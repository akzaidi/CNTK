% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/layers-layers.R
\name{ConvolutionTranspose}
\alias{ConvolutionTranspose}
\title{ConvolutionTranspose}
\usage{
ConvolutionTranspose(filter_shape, num_filters = NULL,
  activation = activation_identity, init = init_glorot_uniform(),
  pad = FALSE, strides = 1, sharing = TRUE, bias = TRUE,
  init_bias = 0, output_shape = NULL, max_temp_mem_size_in_samples = 0,
  reduction_rank = 1, name = "")
}
\description{
Layer factory function to create a convolution transpose layer.
}
\details{
This implements a convolution_transpose operation over items arranged on an
N-dimensional grid, such as pixels in an image. Typically, each item is a
vector (e.g. pixel: R,G,B), and the result is, in turn, a vector. The
item-grid dimensions are referred to as the spatial dimensions (e.g.
dimensions of an image), while the vector dimensions of the individual items
are often called feature-map depth.

Convolution transpose is also known as fractionally strided convolutional
layers, or, deconvolution. This operation is used in image and language
processing applications. It supports arbitrary dimensions, strides, and
padding.

The forward and backward computation of convolution transpose is the inverse
of convolution. That is, during forward pass the input layerâ€™s items are
spread into the output same as the backward spread of gradients in
convolution. The backward pass, on the other hand, performs a convolution
same as the forward pass of convolution.

The size (spatial extent) of the receptive field for convolution transpose
is given by filter_shape. E.g. to specify a 2D convolution transpose,
filter_shape should be a tuple of two integers, such as (5,5); an example
for a 3D convolution transpose (e.g. video or an MRI scan) would be
filter_shape=(3,3,3); while for a 1D convolution transpose (e.g. audio or
text), filter_shape has one element, such as (3,).

The dimension of the input items (feature-map depth) is not specified, but
known from the input. The dimension of the output items generated for each
item position is given by num_filters.

A ConvolutionTranspose instance owns its weight parameter tensors W and b,
and exposes them as an attributes .W and .b. The weights will have the shape
(input_feature_map_depth, num_filters, *filter_shape).
}
